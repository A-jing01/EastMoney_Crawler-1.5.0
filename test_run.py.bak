import logging
import logging.config
import time

# 尝试加载 logging.conf，否则回退到 basicConfig 写 crawler.log
try:
    logging.config.fileConfig('logging.conf', disable_existing_loggers=False)
except Exception:
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(threadName)s - %(message)s",
        handlers=[
            logging.FileHandler("crawler.log", encoding="utf-8"),
            logging.StreamHandler()
        ]
    )

logger = logging.getLogger(__name__)
logger.info("=== 启动 test_run.py（将实例化 PostCrawler 并抓取 1 页） ===")

from crawler import PostCrawler

# small test: headless=True 更不易被系统窗口/焦点打断
p = PostCrawler('000333', headless=True)
p.crawl_post_info(1, 1)

logger.info("=== test_run.py 完成 ===")

# python 一次性运行以创建唯一索引
from pymongo import MongoClient
c = MongoClient("localhost", 27017)
db = c["post_info"]
db["post_000333"].create_index("post_url", unique=True, background=True)
print("index created")
